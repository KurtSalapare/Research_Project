{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: playwright\n",
      "Version: 1.52.0\n",
      "Summary: A high-level API to automate web browsers\n",
      "Home-page: https://github.com/Microsoft/playwright-python\n",
      "Author: Microsoft Corporation\n",
      "Author-email: \n",
      "License-Expression: Apache-2.0\n",
      "Location: C:\\Users\\Kurt Salapare\\Desktop\\TCS_1\\Research Project\\research-project-source-code\\venv\\Lib\\site-packages\n",
      "Requires: greenlet, pyee\n",
      "Required-by: Crawl4AI, tf-playwright-stealth\n"
     ]
    }
   ],
   "source": [
    "# !pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Ollama Qwen Model**\n",
    "\n",
    "Trying simple extraction of just the body of a site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Using ollama.chat ---\n",
      "Chat Response:\n",
      "As an AI language model, I cannot access external websites directly. However, based on the provided URL, I can summarize the paragraphs from the website:\n",
      "\n",
      "1. Introduction to the topic:\n",
      "   \"AI and chatbots are increasingly being used in customer service for automotive dealerships. This article highlights the potential perils of using AI for customer service in this context.\"\n",
      "\n",
      "2. Example: A Chevrolet for $1\n",
      "   \"The example given is a fictional car listing on a dealership's website. The AI chatbot would assist customers by asking questions about their preferences and budget. However, the headline 'A Chevrolet for $1' implies a seemingly unreasonable price, which could be an AI-generated error.\"\n",
      "\n",
      "3. Risks and perils of AI in customer service:\n",
      "   \"Using AI in customer service can bring efficiency and personalized experiences. However, it also poses risks such as miscommunication or errors due to AI's reliance on algorithms and data inputs.\"\n",
      "\n",
      "4. Conclusion and future implications:\n",
      "   \"In conclusion, while AI and chatbots offer significant benefits for automotive customer service, they must be used judiciously to minimize potential pitfalls. As AI continues to advance, the role of AI in customer service will undoubtedly evolve.\"\n",
      "\n",
      "==============================\n",
      "\n",
      "--- Using ollama.generate ---\n",
      "Generate Response:\n",
      "Sure, here are all the paragraphs from the website you provided:\n",
      "\n",
      "1. \"As AI becomes more prevalent in customer service, it's important to understand the potential risks involved.\"\n",
      "\n",
      "2. \"Recently, a car dealership experienced a humorous yet cautionary AI issue. For just one car sold, the dealer had to chat with a bot that couldn't actually sell the vehicle.\"\n",
      "\n",
      "3. \"This example highlights the dangers of relying solely on AI for customer service tasks. In situations where key actions are required, human intervention is still necessary.\"\n",
      "\n",
      "4. \"While AI can certainly streamline and enhance customer support processes, it's crucial to strike a balance between automation and human interaction.\"\n",
      "\n",
      "In summary, the paragraphs provide insights into the potential risks and perils of using AI for customer service, along with emphasizing the importance of maintaining a balance between automation and human interaction.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "def run_qwen_ollama(prompt_text):\n",
    "  \"\"\"\n",
    "  Sends a prompt to the Qwen model in Ollama and prints the response.\n",
    "\n",
    "  Args:\n",
    "    prompt_text: The text prompt to send to the model.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    # Initialize the Ollama client\n",
    "    client = ollama.Client()\n",
    "\n",
    "    # --- Using the chat API (recommended for conversational turns) ---\n",
    "    print(\"--- Using ollama.chat ---\")\n",
    "    messages = [\n",
    "        {'role': 'user', 'content': prompt_text},\n",
    "    ]\n",
    "    response_chat = client.chat(model='qwen:7b', messages=messages)\n",
    "    print(\"Chat Response:\")\n",
    "    print(response_chat['message']['content'])\n",
    "\n",
    "    print(\"\\n\" + \"=\"*30 + \"\\n\") # Separator\n",
    "\n",
    "    # --- Using the generate API (simpler for single prompts) ---\n",
    "    print(\"--- Using ollama.generate ---\")\n",
    "    response_generate = client.generate(model='qwen:7b', prompt=prompt_text)\n",
    "    print(\"Generate Response:\")\n",
    "    print(response_generate['response'])\n",
    "\n",
    "\n",
    "  except ollama.ResponseError as e:\n",
    "    print(f\"Error interacting with Ollama: {e}\")\n",
    "    print(\"Please ensure Ollama is running and the 'qwen:7b' model is pulled.\")  \n",
    "  except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "  my_prompt = \"\"\"\n",
    "  Look at the website attached to this url: https://venturebeat.com/ai/a-chevy-for-1-car-dealer-chatbots-show-perils-of-ai-for-customer-service/, \n",
    "  and return all the paragraphs from the website.\n",
    "  \"\"\"\n",
    "  run_qwen_ollama(my_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Testing With Beautiful Soup***\n",
    "\n",
    "Scraping with beautifulsoup python lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head><title>403 Forbidden</title></head>\n",
      "<body>\n",
      "<center><h1>403 Forbidden</h1></center>\n",
      "<hr/><center>nginx</center>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bsoup\n",
    "import requests\n",
    "\n",
    "headers = {\n",
    "    'User Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36',\n",
    "    'Referer' : 'https://hiddenlayer.com/innovation-hub/novel-universal-bypass-for-all-major-llms/'\n",
    "}\n",
    "\n",
    "page_to_scrape = requests.get(\"https://hiddenlayer.com/innovation-hub/novel-universal-bypass-for-all-major-llms/\", headers=headers)\n",
    "html_soup = bsoup(page_to_scrape.text, \"html.parser\")\n",
    "print(html_soup)\n",
    "# soup = bsoup(html_soup, \"lxml\") \n",
    "# div_tag = html_soup.find(\"div\", class_=\"entry-content-wrapper\")\n",
    "# paragraphs = [p_tag.get_text() for p_tag in div_tag.find_all(\"p\")]\n",
    "# print(paragraphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Testing with Crawl4ai***\n",
    "\n",
    "Webscraping with Crawl4ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "# Ensure crawl4ai is imported correctly as per previous corrections\n",
    "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n",
    "import ollama\n",
    "\n",
    "# --- Configuration ---\n",
    "OLLAMA_MODEL = 'qwen:7b' # Or 'qwen:latest', 'qwen:14b', etc.\n",
    "MAX_LLM_INPUT_CHARS = 15000 # Max characters for LLM input to avoid context window issues\n",
    "                             # Adjust based on your Qwen model's actual context window.\n",
    "\n",
    "# --- 1. Function to crawl the URL and get text content using crawl4ai (Remains the same) ---\n",
    "async def get_webpage_content_with_crawl4ai(url: str):\n",
    "    \"\"\"\n",
    "    Crawls a given URL using crawl4ai and returns its raw Markdown content.\n",
    "    \"\"\"\n",
    "    browser_conf = BrowserConfig(\n",
    "        browser_type=\"chromium\", # Use \"chromium\" for Chrome/Chromium-based browsers\n",
    "        headless=False,          # Set to True for running without a visible browser window\n",
    "        verbose=True             # Set to True for more detailed logging from the browser\n",
    "    )\n",
    "    run_conf = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)\n",
    "\n",
    "    print(f\"[Crawl4AI] Starting crawl for: {url}\")\n",
    "    # try:\n",
    "    #     print('RESULT UNDER')\n",
    "    #     async with AsyncWebCrawler(config=browser_conf) as crawler:\n",
    "    #         result = await crawler.arun(url=url, config=run_conf)\n",
    "    #         print(result)\n",
    "\n",
    "    #     if result.success:\n",
    "    #         print(f\"[Crawl4AI] Successfully crawled {url}. Returning markdown content.\")\n",
    "    #         return result.markdown_v2.raw_markdown\n",
    "    #     else:\n",
    "    #         print(f\"[Crawl4AI] Crawl failed for {url}: {result.error_message}\")\n",
    "    #         return None\n",
    "    # except Exception as e:\n",
    "    #     print(f\"[Crawl4AI] An unexpected error during crawl4ai execution for {url}: {e}\")\n",
    "    #     return None\n",
    "    \n",
    "    print('RESULT UNDER')\n",
    "    async with AsyncWebCrawler(config=browser_conf) as crawler:\n",
    "        result = await crawler.arun(url=url, config=run_conf)\n",
    "        print(result)\n",
    "\n",
    "    if result.success:\n",
    "        print(f\"[Crawl4AI] Successfully crawled {url}. Returning markdown content.\")\n",
    "        return result.markdown_v2.raw_markdown\n",
    "    else:\n",
    "        print(f\"[Crawl4AI] Crawl failed for {url}: {result.error_message}\")\n",
    "        return None\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Function to process text with Ollama/Qwen (MODIFIED for plain string output) ---\n",
    "async def extract_paragraphs_with_qwen(text_content: str, model: str = OLLAMA_MODEL) -> list[str]:\n",
    "    \"\"\"\n",
    "    Uses Ollama/Qwen to extract only coherent text paragraphs from the given content.\n",
    "    Instructs Qwen to return paragraphs separated by two newlines, then splits them.\n",
    "    \"\"\"\n",
    "    if not text_content:\n",
    "        return []\n",
    "\n",
    "    client = ollama.Client()\n",
    "\n",
    "    # Limit the input text length to fit within the LLM's context window\n",
    "    if len(text_content) > MAX_LLM_INPUT_CHARS:\n",
    "        print(f\"[Qwen] Truncating content to {MAX_LLM_INPUT_CHARS} characters for LLM processing.\")\n",
    "        text_content = text_content[:MAX_LLM_INPUT_CHARS]\n",
    "\n",
    "    # Adjusted Prompt: Asking for newline-separated paragraphs, no JSON format\n",
    "    prompt_messages = [\n",
    "        {'role': 'system', 'content': 'You are an expert web content extractor. Your task is to identify and extract only the main, coherent text paragraphs from the provided web page content. Exclude all non-paragraph elements such as headers, footers, navigation links, advertisements, image captions, code blocks, or short, disconnected phrases. Return the extracted paragraphs as a single string, with each paragraph separated by exactly two newline characters (`\\n\\n`). Do NOT include any introductory or concluding remarks, explanations, or additional text from yourself.'},\n",
    "        {'role': 'user', 'content': f\"Extract all text paragraphs from the following web page content:\\n\\n{text_content}\"}\n",
    "    ]\n",
    "\n",
    "    print(f\"[Qwen] Sending content to {model} for paragraph extraction (plain text response expected)...\")\n",
    "    try:\n",
    "        # Removed format='json' from here\n",
    "        response = await asyncio.to_thread(\n",
    "            lambda: client.chat(model=model, messages=prompt_messages, options={'temperature': 0.1})\n",
    "        )\n",
    "\n",
    "        qwen_output_str = response['message']['content']\n",
    "        print(f\"[Qwen] Received raw response from {model}. Splitting into paragraphs.\")\n",
    "\n",
    "        # Split the string by two newlines and strip whitespace from each part\n",
    "        # Filter out empty strings that might result from extra newlines or no content\n",
    "        paragraphs_list = [p.strip() for p in qwen_output_str.split('\\n\\n') if p.strip()]\n",
    "\n",
    "        return paragraphs_list\n",
    "\n",
    "    except ollama.ResponseError as e:\n",
    "        print(f\"[Qwen] Error interacting with Ollama ({model}): {e}\")\n",
    "        print(\"Please ensure Ollama is running and the model is pulled.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"[Qwen] An unexpected error occurred during Ollama processing: {e}\")\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main execution flow ---\n",
    "async def main():\n",
    "    # --- IMPORTANT: Replace with the actual URL you want to scrape ---\n",
    "    # Always check the website's robots.txt and terms of service before scraping.\n",
    "    # target_url = \"https://www.theverge.com/2024/5/15/24157147/openai-gpt-4o-voice-mode-safety-concerns\"\n",
    "    # Example for a more structured site:\n",
    "    target_url = \"https://www.bbc.com/news/articles/cn9j1052p2yo\"\n",
    "\n",
    "    print(f\"Starting web scraping and paragraph extraction for: {target_url}\")\n",
    "\n",
    "    # Step 1: Crawl the website to get its content\n",
    "    web_content = await get_webpage_content_with_crawl4ai(target_url)\n",
    "\n",
    "    if web_content:\n",
    "        # Step 2: Use Qwen to extract paragraphs from the crawled content\n",
    "        extracted_paragraphs = await extract_paragraphs_with_qwen(web_content, model=OLLAMA_MODEL)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "        print(\"--- Final Extracted Paragraphs ---\")\n",
    "        if extracted_paragraphs:\n",
    "            print(f\"Successfully extracted {len(extracted_paragraphs)} paragraphs:\")\n",
    "            for i, paragraph in enumerate(extracted_paragraphs):\n",
    "                print(f\"--- Paragraph {i+1} ---\")\n",
    "                print(paragraph)\n",
    "                print(\"-\" * 20) # Simple separator\n",
    "            print(\"\\n--- Process Completed ---\")\n",
    "            return extracted_paragraphs\n",
    "        else:\n",
    "            print(\"Qwen processing returned no paragraphs.\")\n",
    "    else:\n",
    "        print(\"Failed to get web page content with crawl4ai.\")\n",
    "\n",
    "    return []\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "# Corrected import statement\n",
    "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n",
    "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n",
    "import json\n",
    "\n",
    "async def main():\n",
    "    # 1. Browser config\n",
    "    browser_cfg = BrowserConfig(\n",
    "        browser_type=\"chromium\", # Or \"chromium\" for Chrome\n",
    "        headless=True\n",
    "    )\n",
    "\n",
    "\n",
    "    run_cfg = CrawlerRunConfig(\n",
    "        cache_mode=CacheMode.BYPASS,\n",
    "    )\n",
    "\n",
    "    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n",
    "        result = await crawler.arun(\n",
    "            url=\"https://www.bbc.com/news/articles/cn9j1052p2yo\", # Replace with your target URL\n",
    "            config=run_cfg\n",
    "        )\n",
    "\n",
    "        if result.success:\n",
    "            print(\"Cleaned HTML length:\", len(result.cleaned_html))\n",
    "            if result.extracted_content:\n",
    "                articles = json.loads(result.extracted_content)\n",
    "                print(\"Extracted articles:\", articles[:2])\n",
    "            else:\n",
    "                print(\"No content extracted despite success.\")\n",
    "        else:\n",
    "            print(\"Error:\", result.error_message)\n",
    "\n",
    "# --- How to run it in a Jupyter Notebook cell ---\n",
    "# Simply await the main() function directly\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-8' coro=<Connection.run() done, defined at c:\\Users\\Kurt Salapare\\Desktop\\TCS_1\\Research Project\\research-project-source-code\\venv\\Lib\\site-packages\\playwright\\_impl\\_connection.py:272> exception=NotImplementedError()>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Kurt Salapare\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Kurt Salapare\\Desktop\\TCS_1\\Research Project\\research-project-source-code\\venv\\Lib\\site-packages\\playwright\\_impl\\_connection.py\", line 279, in run\n",
      "    await self._transport.connect()\n",
      "  File \"c:\\Users\\Kurt Salapare\\Desktop\\TCS_1\\Research Project\\research-project-source-code\\venv\\Lib\\site-packages\\playwright\\_impl\\_transport.py\", line 133, in connect\n",
      "    raise exc\n",
      "  File \"c:\\Users\\Kurt Salapare\\Desktop\\TCS_1\\Research Project\\research-project-source-code\\venv\\Lib\\site-packages\\playwright\\_impl\\_transport.py\", line 120, in connect\n",
      "    self._proc = await asyncio.create_subprocess_exec(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Kurt Salapare\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\subprocess.py\", line 224, in create_subprocess_exec\n",
      "    transport, protocol = await loop.subprocess_exec(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Kurt Salapare\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 1756, in subprocess_exec\n",
      "    transport = await self._make_subprocess_transport(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Kurt Salapare\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 528, in _make_subprocess_transport\n",
      "    raise NotImplementedError\n",
      "NotImplementedError\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     36\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     37\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mError:\u001b[39m\u001b[33m\"\u001b[39m, result.error_message)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kurt Salapare\\Desktop\\TCS_1\\Research Project\\research-project-source-code\\venv\\Lib\\site-packages\\nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kurt Salapare\\Desktop\\TCS_1\\Research Project\\research-project-source-code\\venv\\Lib\\site-packages\\nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\futures.py:202\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     13\u001b[39m browser_cfg = BrowserConfig(\n\u001b[32m     14\u001b[39m     browser_type=\u001b[33m\"\u001b[39m\u001b[33mchromium\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m     headless=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     16\u001b[39m     verbose=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     17\u001b[39m )\n\u001b[32m     19\u001b[39m run_cfg = CrawlerRunConfig(\n\u001b[32m     20\u001b[39m     cache_mode=CacheMode.BYPASS,\n\u001b[32m     21\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m AsyncWebCrawler(config=browser_cfg) \u001b[38;5;28;01mas\u001b[39;00m crawler:\n\u001b[32m     24\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m crawler.arun(\n\u001b[32m     25\u001b[39m         url=\u001b[33m\"\u001b[39m\u001b[33mhttps://www.bbc.com/news/articles/cvgddel17kvo\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     26\u001b[39m         config=run_cfg\n\u001b[32m     27\u001b[39m     )\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result.success:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kurt Salapare\\Desktop\\TCS_1\\Research Project\\research-project-source-code\\venv\\Lib\\site-packages\\crawl4ai\\async_webcrawler.py:191\u001b[39m, in \u001b[36mAsyncWebCrawler.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.start()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kurt Salapare\\Desktop\\TCS_1\\Research Project\\research-project-source-code\\venv\\Lib\\site-packages\\crawl4ai\\async_webcrawler.py:174\u001b[39m, in \u001b[36mAsyncWebCrawler.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    168\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    169\u001b[39m \u001b[33;03m    Start the crawler explicitly without using context manager.\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[33;03m    This is equivalent to using 'async with' but gives more control over the lifecycle.\u001b[39;00m\n\u001b[32m    171\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[33;03m        AsyncWebCrawler: The initialized crawler instance\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.crawler_strategy.\u001b[34m__aenter__\u001b[39m()\n\u001b[32m    175\u001b[39m     \u001b[38;5;28mself\u001b[39m.logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCrawl4AI \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcrawl4ai_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, tag=\u001b[33m\"\u001b[39m\u001b[33mINIT\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    176\u001b[39m     \u001b[38;5;28mself\u001b[39m.ready = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kurt Salapare\\Desktop\\TCS_1\\Research Project\\research-project-source-code\\venv\\Lib\\site-packages\\crawl4ai\\async_crawler_strategy.py:111\u001b[39m, in \u001b[36mAsyncPlaywrightCrawlerStrategy.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.start()\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kurt Salapare\\Desktop\\TCS_1\\Research Project\\research-project-source-code\\venv\\Lib\\site-packages\\crawl4ai\\async_crawler_strategy.py:121\u001b[39m, in \u001b[36mAsyncPlaywrightCrawlerStrategy.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    118\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    119\u001b[39m \u001b[33;03m    Start the browser and initialize the browser manager.\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.browser_manager.start()\n\u001b[32m    122\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.execute_hook(\n\u001b[32m    123\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mon_browser_created\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    124\u001b[39m         \u001b[38;5;28mself\u001b[39m.browser_manager.browser,\n\u001b[32m    125\u001b[39m         context=\u001b[38;5;28mself\u001b[39m.browser_manager.default_context,\n\u001b[32m    126\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kurt Salapare\\Desktop\\TCS_1\\Research Project\\research-project-source-code\\venv\\Lib\\site-packages\\crawl4ai\\browser_manager.py:593\u001b[39m, in \u001b[36mBrowserManager.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    589\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplaywright\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01masync_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m async_playwright\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m \u001b[38;5;28mself\u001b[39m.playwright = \u001b[38;5;28;01mawait\u001b[39;00m async_playwright().start()\n\u001b[32m    595\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.cdp_url \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_managed_browser:\n\u001b[32m    596\u001b[39m     \u001b[38;5;28mself\u001b[39m.config.use_managed_browser = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kurt Salapare\\Desktop\\TCS_1\\Research Project\\research-project-source-code\\venv\\Lib\\site-packages\\playwright\\async_api\\_context_manager.py:51\u001b[39m, in \u001b[36mPlaywrightContextManager.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> AsyncPlaywright:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__aenter__\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kurt Salapare\\Desktop\\TCS_1\\Research Project\\research-project-source-code\\venv\\Lib\\site-packages\\playwright\\async_api\\_context_manager.py:46\u001b[39m, in \u001b[36mPlaywrightContextManager.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m playwright_future.done():\n\u001b[32m     45\u001b[39m     playwright_future.cancel()\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m playwright = AsyncPlaywright(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     47\u001b[39m playwright.stop = \u001b[38;5;28mself\u001b[39m.\u001b[34m__aexit__\u001b[39m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m playwright\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\futures.py:202\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Kurt Salapare\\Desktop\\TCS_1\\Research Project\\research-project-source-code\\venv\\Lib\\site-packages\\playwright\\_impl\\_transport.py:120\u001b[39m, in \u001b[36mPipeTransport.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    117\u001b[39m         startupinfo.wShowWindow = subprocess.SW_HIDE\n\u001b[32m    119\u001b[39m     executable_path, entrypoint_path = compute_driver_executable()\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[38;5;28mself\u001b[39m._proc = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_subprocess_exec(\n\u001b[32m    121\u001b[39m         executable_path,\n\u001b[32m    122\u001b[39m         entrypoint_path,\n\u001b[32m    123\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrun-driver\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    124\u001b[39m         stdin=asyncio.subprocess.PIPE,\n\u001b[32m    125\u001b[39m         stdout=asyncio.subprocess.PIPE,\n\u001b[32m    126\u001b[39m         stderr=_get_stderr_fileno(),\n\u001b[32m    127\u001b[39m         limit=\u001b[32m32768\u001b[39m,\n\u001b[32m    128\u001b[39m         env=env,\n\u001b[32m    129\u001b[39m         startupinfo=startupinfo,\n\u001b[32m    130\u001b[39m     )\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    132\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_error_future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\subprocess.py:224\u001b[39m, in \u001b[36mcreate_subprocess_exec\u001b[39m\u001b[34m(program, stdin, stdout, stderr, limit, *args, **kwds)\u001b[39m\n\u001b[32m    221\u001b[39m loop = events.get_running_loop()\n\u001b[32m    222\u001b[39m protocol_factory = \u001b[38;5;28;01mlambda\u001b[39;00m: SubprocessStreamProtocol(limit=limit,\n\u001b[32m    223\u001b[39m                                                     loop=loop)\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m transport, protocol = \u001b[38;5;28;01mawait\u001b[39;00m loop.subprocess_exec(\n\u001b[32m    225\u001b[39m     protocol_factory,\n\u001b[32m    226\u001b[39m     program, *args,\n\u001b[32m    227\u001b[39m     stdin=stdin, stdout=stdout,\n\u001b[32m    228\u001b[39m     stderr=stderr, **kwds)\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Process(transport, protocol, loop)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py:1756\u001b[39m, in \u001b[36mBaseEventLoop.subprocess_exec\u001b[39m\u001b[34m(self, protocol_factory, program, stdin, stdout, stderr, universal_newlines, shell, bufsize, encoding, errors, text, *args, **kwargs)\u001b[39m\n\u001b[32m   1754\u001b[39m     debug_log = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mexecute program \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprogram\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m   1755\u001b[39m     \u001b[38;5;28mself\u001b[39m._log_subprocess(debug_log, stdin, stdout, stderr)\n\u001b[32m-> \u001b[39m\u001b[32m1756\u001b[39m transport = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_subprocess_transport(\n\u001b[32m   1757\u001b[39m     protocol, popen_args, \u001b[38;5;28;01mFalse\u001b[39;00m, stdin, stdout, stderr,\n\u001b[32m   1758\u001b[39m     bufsize, **kwargs)\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._debug \u001b[38;5;129;01mand\u001b[39;00m debug_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1760\u001b[39m     logger.info(\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m'\u001b[39m, debug_log, transport)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py:528\u001b[39m, in \u001b[36mBaseEventLoop._make_subprocess_transport\u001b[39m\u001b[34m(self, protocol, args, shell, stdin, stdout, stderr, bufsize, extra, **kwargs)\u001b[39m\n\u001b[32m    524\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_make_subprocess_transport\u001b[39m(\u001b[38;5;28mself\u001b[39m, protocol, args, shell,\n\u001b[32m    525\u001b[39m                                      stdin, stdout, stderr, bufsize,\n\u001b[32m    526\u001b[39m                                      extra=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    527\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create subprocess transport.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import asyncio\n",
    "\n",
    "from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode\n",
    "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n",
    "import json\n",
    "\n",
    "async def main():\n",
    "    # 1. Browser config\n",
    "    browser_cfg = BrowserConfig(\n",
    "        browser_type=\"chromium\",\n",
    "        headless=False,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    run_cfg = CrawlerRunConfig(\n",
    "        cache_mode=CacheMode.BYPASS\n",
    "    )\n",
    "\n",
    "    async with AsyncWebCrawler(config=browser_cfg) as crawler:\n",
    "        result = await crawler.arun(\n",
    "            url=\"https://www.bbc.com/news/articles/cvgddel17kvo\",\n",
    "            config=run_cfg\n",
    "        )\n",
    "\n",
    "        if result.success:\n",
    "            print(\"Cleaned HTML length:\", len(result.cleaned_html))\n",
    "            print(result.cleaned_html)\n",
    "            print(result)\n",
    "            if result.extracted_content:\n",
    "                articles = json.loads(result.extracted_content)\n",
    "                print(\"Extracted articles:\", articles[:2])\n",
    "        else:\n",
    "            print(\"Error:\", result.error_message)\n",
    "\n",
    "asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
